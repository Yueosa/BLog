---
title: 一个“玩具项目”的诞生
subtitle: 将 LLM 当作函数后, 我终于认识了 Agent
date: 2026-01-27
updated: 2026-01-27
categories:
  - 学习
tags:
  - LLM
  - Agent
  - RAG
  - Prompt
  - FSM
  - MCP
---

> **我在学校做的毕业设计题目为 "基于 MCP 协议的多专家协作智能体系统设计与实现"**
>
> Github 地址为 [Lian-MCP-LLM-Agent](https://github.com/Yueosa/Lian-MCP-LLM-Agent)
>
> 其实算是我写的一个玩具项目, 目前因为生活繁忙暂停开发
>
> 但开发过程中确确实实学到了很多有用的东西, 也出现了很多有意思的想法
>
> 所以打算写一篇博客分享出来

---

## 🤖 LLM 的本质

> **大语言模型 (Large Language Model)** 是一种基于 **Transformer** 架构的深度学习 **AI**, 透过数十亿至数百亿的参数训练海量文字数据
>
> 他擅长理解、总结、翻译、生成文本与代码
>
> LLM 利用自注意力机制分析上下文并预测下一个字词, 是目前生成式 AI 的核心技术

用我的理解来说的话, LLM 的本质其实就是:

```rust
fn llm(input: Text, context: Text) -> Text
```
即: **根据用户输入与上下文信息, 生成最可能的下一个字符串**

### 🌰 打个比方

**LLM 生成的每一个字, 都是从无数个岔路口中选择一个, 连续选择 `10` 次, 他就给你生成了一句 10 个字的话**

而 **模型结构 (Transformer)**、**参数 (权重)**、**训练数据分布** 共同决定了:

*   这些分岔路有多复杂
*   长什么样
*   哪些路比较宽

### 🌲 LLM 的局部最优问题

沿用上述的分岔路模型, 想象这么一棵树:

```text
起点
 ├── A（现在看起来很好）
 │    ├── A1（还行）
 │    └── A2（死路）
 └── B（现在看起来一般）
      └── B1（远处是宝藏）
```

LLM 在第一个岔路口, **他看不到终点**, 只能根据当前上下文的概率分布选。

于是 **A 的概率高**, **B 的概率低**, 他就会选 **A**。

但真正的 "全局最优" 是 `B -> B1`。

#### 🤔 为什么 LLM 的最终结果看起来是优秀的?

因为在训练数据里存在 **统计巧合** + **人类经验复用**。

一旦问题 **太新**、**太长**、**太需要因果**, LLM 就会暴露本性。

> 这么看起来 LLM 的缺点实在是太多了喵!
>
> *   **无法感知真实世界的信息流动**
> *   **没办法长期坚持做一件具体的事**
> *   **不能理解因果链条**
> *   ...

---

## 📚 RAG 技术

> 如何从原理上让 LLM 更好用? 也许需要对 `context` 动动手脚

日常使用 LLM 对话时最困扰我的问题就是: **这个平台有没有长期记忆库功能?**

---

## 🪄 Prompt 工程

> 让 LLM 更好用的另一个原理级武器: `user_input`

如果想要 LLM 生成我想要的格式, 成本最低的方式就是写一堆非常复杂的 Prompt ...

---

## ⚙️ 状态机

> 状态机的核心其实是一场 SM 调教 ...

通过限制整个 Agent 的权限, 让他从 "本能" 上 "**只会做某些事**" 就好啦 ~

---

## 🔌 MCP

> 我理解的 MCP: 一种万能对象互联协议

我平时写代码就特别喜欢把函数的参数返回规范成结构体, 这个思想自然被带到了 Agent 开发里 ~

---
